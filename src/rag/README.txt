1. CharacterTextSplitter（字符分割器）按指定字符分割（默认是\n\n，即空行），分割后按chunk_size切分，是最基础、最通用的分割器。
2. RecursiveCharacterTextSplitter（递归字符分割器）优先级分割符递归分割（默认：\n\n → \n → → ``），
先按大分割符切分，若块仍过大，再用小分割符切分，最大程度保证语义完整性
3. MarkdownTextSplitter（Markdown 分割器）:按Markdown 的语法结构分割（如# 标题、## 二级标题、### 三级标题、
``` 代码块、> 引用），按标题层级切分，保证每个文本块属于同一个主题，是 Markdown 文档的最优解。
4. PythonCodeTextSplitter/JavaScriptCodeTextSplitter/HTMLTextSplitter（代码分割器）按编程语言的语法结构分割
（如函数、类、代码块、注释），针对 Python/JS 代码做了优化，不会切断函数 / 类的定义。
5. TokenTextSplitter（Token 分割器）：直接按 token 数分割，无需自定义长度函数，底层基于 HuggingFace 的tokenizers库，
精准控制每个文本块的 token 数，完全匹配嵌入模型的输入限制。
6. SemanticChunker（语义分割器）基于语义相似度的智能分割器，不是按固定字符 /token 分割，而是通过计算句子间的语义相似度，
将语义相近的句子归为一个块，语义差异大的地方分割，最大程度保证语义完整性。


嵌入式文本的维度：
模型类型	代表模型	向量维度	适用场景
轻量中文模型	text2vec-base-chinese	768	中小规模 RAG、低资源服务器
轻量中文模型（升级版）	text2vec-large-chinese	1024	中规模 RAG、平衡效果 / 速度
BERT 系列基础版	bert-base-uncased	768	英文场景、通用嵌入
BERT 系列大模型	bert-large-uncased	1024	英文场景、更高精度需求
Sentence-BERT（SBERT）	all-MiniLM-L6-v2	384	轻量英文场景、高并发
Sentence-BERT（SBERT）	all-mpnet-base-v2	768	英文场景、主流选择

768 维比 384 维能更好地区分近义词 / 歧义句（如 “苹果手机” 和 “苹果水果”）；
1024 维能捕捉更复杂的长文本逻辑（如技术文档、论文的段落关联）。
小场景 / 短文本：384 维（如 all-MiniLM-L6-v2）足够用，速度快、成本低，效果和 768 维差距极小；
中大型场景 / 专业领域 / 长文本：768/1024 维是性价比之选，能平衡表征能力和成本；
超大规模维度（如 2048+）：非特殊需求不建议用，除非有专门的降维优化和高性能硬件。

优化文档处理环节：
合理切分文档：按语义块切分（如段落、小节）、
清洗无效内容（广告、水印、空行、重复内容）、
添加文档元信息：切分后给每个文本块加元信息
开启向量归一化：能大幅提升余弦相似度的计算精度
统一文本格式：统一格式（如全小写、去除特殊符号、统一标点）
多路检索 + 重排序：向量检索找到 Top20 的文本块，再关键词检索（BM25） 找到 Top20，合并后用重排序模型（如 cross-encoder）
调整检索 TopK 值：根据文档长度和场景调整 TopK，太小会漏掉关键信息，太大会引入噪声
使用高性能向量数据库：替代轻量的 Chroma，用 FAISS（单机高性能）、PGVector（结合 PostgreSQL）、Milvus（分布式）


# 1.向量数据库 + 检索（最常见）
将对话信息转换为向量，存入像FAISS、Milvus等向量数据库中，当模型需要”回忆“时，就向数据库中检索相关信息，并将结果加到模型输入的prompt中。
部署起来会相对简单,扩展性也会很强,准确度低的问题，缺点是在存储过程中可能无法理解”时间顺序“或”语境依赖“导致记忆中忽略这部分的信息  

# 2. Slot-based 记忆管理（插槽式记忆）
小明是重庆人，特别喜欢吃辣椒”。那么其结构化存储：
●用户姓名：小明
●喜好：爱吃辣
●背景信息：重庆人  
更加结构化，方便了记忆存储与记忆召回
需要手动设定插槽，灵活性与兼容性很差

# 3. 多轮对话链 + 自动总结（总结记忆）
让模型定期地对过去的对话进行“反思”，存储对话中”高度抽象“的信息，而非其原始内容。
缺乏细节，会影响记忆召回的精确度。

# 4. 混合式：当前最主流的解决方案
●用 向量数据库存原始记忆片段
●用 slot 存储结构化长期信息（如角色设定、兴趣偏好）
●用 总结机制压缩上下文，提高效率

mem0：轻量、实用、以用为先的记忆系统
✅ 记忆是可搜索和可管理的：通过自然语言索引+向量化混合检索。「向量库 + 弹性检索」
✅ 支持多记忆分层结构：如“人物档案”、“事件记录”、“标签主题”等。「结构化存储」
✅ 支持自动摘要与反思机制：模型定期总结近期对话，形成更稳固的记忆基础。「总结记忆」
✅ 支持“记忆触发器”机制：当对话触发某关键词或语义线索时，自动检索相关记忆。
【原始对话数据】→ 纯文本存储（轻量数据库/本地文件）
【结构化记忆】→ 结构化存储（轻量关系库/JSON）
【总结记忆】→ 混合存储（文本+向量）
          ↓ 【唯一ID绑定层】（所有记忆挂同一个ID，实现联动）
【核心向量库】→ 存储「原始/结构化/总结记忆」的嵌入向量（仅存向量，不存原文）
具体操作实现：
1. 原始对话记忆：对零散对话原文单独向量化
原始文本（存纯文本存储）：用户问：RAG索引链怎么长期记忆？答：用向量库+关系库持久化，唯一ID绑定
向量化：把上面这整段原文文本丢进 Embedding 模型，生成一个原始记忆向量 V1
向量库存入：ID=rag_001 | 类型=原始记忆 | 向量=V1
2. 结构化记忆：对结构化数据的文本化内容单独向量化
结构化记忆不是直接丢 JSON 向量化（模型不认纯 JSON），会先转成自然语言文本，再单独生成向量：
结构化数据（存结构化存储）：{人物:程序员, 主题:RAG, 核心问题:索引链长期记忆, 标签:AI开发}
文本化转换：程序员咨询RAG相关问题，核心是索引链的长期记忆，标签为AI开发
向量化：把转换后的结构化文本丢进 Embedding 模型，生成一个结构化记忆向量 V2
向量库存入：ID=rag_001 | 类型=结构化记忆 | 向量=V2
3. 总结记忆：对浓缩总结的纯文本单独向量化
总结文本（存混合存储）：RAG索引链长期记忆的核心是向量库+关系库持久化，通过唯一ID绑定保证数据一致性
向量化：把上面这总结文本丢进 Embedding 模型，生成一个总结记忆向量 V3
向量库存入：ID=rag_001 | 类型=总结记忆 | 向量=V3

数据类型	存储位置	核心作用
原始文本	MySQL	给 BM25 搜、回溯原文、生成总结
结构化存储	MySQL	精准筛选（按标签 / 人物 / 主题）
三者的嵌入向量	Milvus 向量库	向量语义检索（BGE 模型 + 余弦相似度）
总结记忆	MySQL	快速提供高信息密度的核心记忆

MySQL 存所有 “能看的原文”（原始 / 结构化 / 总结），Milvus 存所有 “对应向量”，靠 doc_id 绑死，各司其职不混存。

MemGPT：类人脑记忆的模拟器
1.Working Memory（工作记忆）：用于当前对话和任务的即时信息，类似人类短期记忆。
2.Long-Term Memory（长期记忆）：存储历史重要信息，随时可检索，类似人类的回忆系统。
它最大的特点是：记忆不是固定插入的，而是由模型自主决定“写入”或“读取”。
●比如，当用户说出一句重要信息，MemGPT 会识别“这值得记住”，并自动存入长期记忆。
●未来对话中，如果触发相关线索，模型会主动“回忆”相关内容并应用到回答中。
这种机制让 AI 更像一个“会反思、有偏好、有选择性记忆”的智能体。