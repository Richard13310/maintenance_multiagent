"""
ç®€åŒ–ç‰ˆ RAG çŸ¥è¯†é—®ç­” - ä»…å¤„ç†PDFæ–‡æœ¬ï¼ˆGraphèŠ‚ç‚¹+BGEä¸­æ–‡æ¨¡å‹+å®˜æ–¹MilvusåŒ…ï¼‰
æœ€ç»ˆç‰ˆï¼šä½¿ç”¨ langchain-milvus åŒ…ï¼Œå½»åº•è§£å†³å…¼å®¹æ€§é—®é¢˜
"""
from typing import List, Dict, Any, Optional, Callable, Tuple
from pathlib import Path
from datetime import datetime
import os

# æ ¸å¿ƒä¾èµ–ï¼ˆä½¿ç”¨å®˜æ–¹æ¨èçš„ langchain-milvus åŒ…ï¼‰
from langchain_community.document_loaders import PyPDFLoader
from langchain_milvus import MilvusVectorStore  # å®˜æ–¹æ–°ç‰ˆMilvuså‘é‡åº“
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.messages import AIMessage, HumanMessage, BaseMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_classic.chains.combine_documents import create_stuff_documents_chain
from langchain_classic.chains.retrieval import create_retrieval_chain
from langchain_core.vectorstores import VectorStoreRetriever

# æ›¿æ¢ä¸ºä½ çš„LLMé…ç½®
from llm_db_config.chatmodel import llm_no_think

# ========== ç¯å¢ƒé…ç½®ï¼ˆå›½å†…é•œåƒ+è¶…æ—¶è®¾ç½®ï¼‰ ==========
os.environ["TRANSFORMERS_OFFLINE"] = "0"
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
os.environ["HF_HUB_CONNECT_TIMEOUT"] = "60"
os.environ["HF_HUB_DOWNLOAD_TIMEOUT"] = "60"

# ========== CUDAè‡ªåŠ¨æ£€æµ‹ ==========
try:
    import torch
    HAS_CUDA = torch.cuda.is_available()
except ImportError:
    HAS_CUDA = False

# ========== é…ç½®ç±»ï¼ˆé€‚é…æ–°ç‰ˆMilvusï¼‰ ==========
class SimpleRAGConfig:
    # Milvusè¿æ¥é…ç½®
    MILVUS_HOST: str = "127.0.0.1"
    MILVUS_PORT: str = "19530"
    COLLECTION_NAME: str = "simple_pdf_rag_bge"  # å‘é‡é›†åˆå
    # åµŒå…¥æ¨¡å‹é…ç½®ï¼ˆBGEä¸­æ–‡æœ€ä¼˜æ¨¡å‹ï¼‰
    EMBEDDING_MODEL: str = "BAAI/bge-base-zh-v1.5"
    EMBEDDING_DEVICE: str = "cuda" if HAS_CUDA else "cpu"
    # æ£€ç´¢é…ç½®
    SEARCH_K: int = 6  # å¬å›æ–‡æ¡£æ•°
    SEARCH_SCORE_THRESHOLD: float = 0.3  # ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆ0-1ï¼‰
    # æ–‡æœ¬åˆ‡ç‰‡é…ç½®
    CHUNK_SIZE: int = 500
    CHUNK_OVERLAP: int = 50

config = SimpleRAGConfig()

# æ‰“å°è¿è¡Œä¿¡æ¯
print(f"ğŸ”§ å½“å‰è¿è¡Œè®¾å¤‡ï¼š{config.EMBEDDING_DEVICE}")
if not HAS_CUDA:
    print("âš ï¸  æœªæ£€æµ‹åˆ°CUDAï¼Œå°†ä½¿ç”¨CPUè¿è¡Œï¼ˆBGEæ¨¡å‹CPUè¿è¡Œé€Ÿåº¦è¾ƒæ…¢ï¼Œå»ºè®®å®‰è£…GPUç¯å¢ƒï¼‰")

# ========== ç›¸å…³æ€§è¯„åˆ†å‡½æ•°ï¼ˆBGEæ¨¡å‹ä¸“ç”¨ï¼‰ ==========
def cosine_similarity_score_fn(distance: float) -> float:
    """
    ä½™å¼¦ç›¸ä¼¼åº¦è½¬æ¢ï¼šMilvus L2è·ç¦» â†’ ç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆ0-1ï¼‰
    BGEæ¨¡å‹å‘é‡å½’ä¸€åŒ–åï¼ŒL2è·ç¦»èŒƒå›´ä¸º0-2ï¼Œå¯¹åº”ç›¸ä¼¼åº¦1.0-0.0
    """
    return 1.0 - (distance / 2.0)

# ========== RAGæ ¸å¿ƒç±» ==========
class SimplePDFRAGAgent:
    def __init__(self, llm: Any):
        self.llm = llm
        self.embeddings = HuggingFaceEmbeddings(
            model_name=config.EMBEDDING_MODEL,
            model_kwargs={
                "device": config.EMBEDDING_DEVICE,
                "trust_remote_code": True
            },
            encode_kwargs={
                "normalize_embeddings": True  # BGEå¿…é¡»å½’ä¸€åŒ–ï¼Œç¡®ä¿ç›¸ä¼¼åº¦è®¡ç®—å‡†ç¡®
            },
        )
        self.vector_store = MilvusVectorStore(
            embedding_function=self.embeddings,
            connection_args={
                "host": config.MILVUS_HOST,
                "port": config.MILVUS_PORT,
                "alias": "default"  # è¿æ¥åˆ«åï¼ˆæ–°ç‰ˆå¿…å¡«ï¼‰
            },
            collection_name=config.COLLECTION_NAME,
            auto_id=True,  # è‡ªåŠ¨ç”Ÿæˆæ–‡æ¡£ID
            distance_metric="L2",  # ä¸BGEå½’ä¸€åŒ–å‘é‡å…¼å®¹
            drop_old=False,  # æ›¿ä»£æ—§ç‰ˆoverwriteï¼šFalse=ä¸åˆ é™¤æ—§é›†åˆï¼ˆTrue=åˆ é™¤é‡å»ºï¼‰
        )

        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=config.CHUNK_SIZE,
            chunk_overlap=config.CHUNK_OVERLAP,
            separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼›", "ï¼Œ", " "],
        )
        self.retriever = self.vector_store.as_retriever(
            search_kwargs={
                "k": config.SEARCH_K,
                "score_threshold": config.SEARCH_SCORE_THRESHOLD,
                "relevance_score_fn": cosine_similarity_score_fn  # æ˜¾å¼è¯„åˆ†å‡½æ•°
            },
            search_type="similarity_score_threshold",
        )
        self.document_prompt = ChatPromptTemplate.from_messages([
            ("system", """ä½ æ˜¯è®¾å¤‡è¿ç»´åŠ©æ‰‹ï¼Œä¸¥æ ¼åŸºäºæä¾›çš„PDFæ–‡æ¡£å†…å®¹å›ç­”é—®é¢˜ã€‚
                - ä»…ä½¿ç”¨ä¸Šä¸‹æ–‡é‡Œçš„ä¿¡æ¯ï¼Œä¸ç¼–é€ é¢å¤–å†…å®¹
                - æŠ€æœ¯é—®é¢˜æŒ‰ã€Œé—®é¢˜åˆ†æâ†’è§£å†³æ–¹æ¡ˆâ†’æ“ä½œæ­¥éª¤ã€çš„ç»“æ„å›ç­”
                - è‹¥ä¸Šä¸‹æ–‡æ— ç›¸å…³ä¿¡æ¯ï¼Œç›´æ¥å›å¤"æ— æ³•å›ç­”è¯¥é—®é¢˜"
                <context>
                {context}
                </context>"""),
            MessagesPlaceholder(variable_name="chat_history"),
            ("human", "{input}")
        ])
        self.document_chain = create_stuff_documents_chain(
            self.llm,
            self.document_prompt,
            document_prompt=ChatPromptTemplate.from_messages([
                ("system", "[æ–‡æ¡£æ¥æºï¼š{{doc.metadata.source}}] {{doc.page_content}}")
            ])
        )
        self.rag_chain = create_retrieval_chain(self.retriever, self.document_chain, rephrase_question=False) # å…³é—­é—®é¢˜é‡å†™åŠŸèƒ½

    # åŠ è½½PDFå¹¶å…¥åº“
    def load_pdf_to_db(self, pdf_path: str) -> int:
        pdf_path = Path(pdf_path)
        if not pdf_path.exists() or pdf_path.suffix != ".pdf":
            raise ValueError(f"âŒ æ— æ•ˆPDFè·¯å¾„ï¼š{pdf_path}")

        print(f"ğŸ“„ æ­£åœ¨åŠ è½½PDFï¼š{pdf_path.name}")
        loader = PyPDFLoader(str(pdf_path))
        documents = loader.load()
        print(f"âœ‚ï¸ PDFå…±{len(documents)}é¡µï¼Œæ­£åœ¨åˆ‡ç‰‡...")

        split_docs = self.text_splitter.split_documents(documents)
        print(f"âœ… åˆ‡ç‰‡å®Œæˆï¼š{len(split_docs)}ä¸ªæ–‡æ¡£å—")

        # è¡¥å……å…ƒæ•°æ®
        for doc in split_docs:
            doc.metadata.update({
                "load_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "content_type": "text",
                "embedding_model": config.EMBEDDING_MODEL
            })

        # å­˜å…¥Milvus
        print(f"ğŸ“¥ æ­£åœ¨å†™å…¥Milvusé›†åˆï¼š{config.COLLECTION_NAME}")
        self.vector_store.add_documents(split_docs)
        return len(split_docs)

    # é—®ç­”å…¥å£ï¼ˆé€‚é…GraphèŠ‚ç‚¹ï¼‰
    def run(self, state: Dict[str, Any]) -> Dict[str, List[BaseMessage]]:
        messages = state.get("messages", [])
        chat_history = messages[:-1] if len(messages) > 1 else []
        user_input = next((msg.content for msg in reversed(messages) if isinstance(msg, HumanMessage)), "")

        if not user_input:
            return {"messages": [AIMessage(content="æœªè·å–åˆ°æœ‰æ•ˆé—®é¢˜ï¼Œè¯·é‡æ–°è¾“å…¥")]}

        print(f"ğŸ” æ£€ç´¢æŸ¥è¯¢ï¼š{user_input}")
        result = self.rag_chain.invoke({
            "input": user_input,
            "chat_history": chat_history
        })

        return {"messages": [AIMessage(content=result.get("answer", "æ— æ³•å›ç­”è¯¥é—®é¢˜"))]}

    # ç®€åŒ–é—®ç­”æ¥å£
    def ask(self, query: str, chat_history: List[Any] = None) -> str:
        chat_history = chat_history or []
        print(f"ğŸ” æ£€ç´¢æŸ¥è¯¢ï¼š{query}")
        result = self.rag_chain.invoke({
            "input": query,
            "chat_history": chat_history
        })
        return result.get("answer", "æ— æ³•å›ç­”è¯¥é—®é¢˜")

# ========== GraphèŠ‚ç‚¹åˆ›å»ºå‡½æ•°ï¼ˆé€‚é…LangChainï¼‰ ==========
def create_simple_rag_node(llm: Any) -> Callable[[Dict[str, Any], Optional[Dict[str, Any]]], Dict[str, Any]]:
    rag_agent = SimplePDFRAGAgent(llm=llm)

    def rag_node(
        state: Dict[str, Any],
        config: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        return rag_agent.run(state)

    return rag_node

# ========== è¿è¡Œç¤ºä¾‹ ==========
if __name__ == "__main__":
    print("=== å¯åŠ¨RAGé—®ç­”ç³»ç»Ÿï¼ˆBGEä¸­æ–‡æ¨¡å‹+Milvuså‘é‡åº“ï¼‰ ===")
    try:
        # åˆå§‹åŒ–Agent
        rag_agent = SimplePDFRAGAgent(llm=llm_no_think)

        # åŠ è½½PDFï¼ˆç¡®ä¿learn.pdfåœ¨å½“å‰ç›®å½•ï¼‰
        pdf_path = "learn.pdf"
        chunk_count = rag_agent.load_pdf_to_db(pdf_path)
        print(f"âœ… PDFåŠ è½½å®Œæˆï¼å…±{chunk_count}ä¸ªæ–‡æ¡£å—å­˜å…¥Milvus\n")

        # æµ‹è¯•é—®ç­”
        query = "è®¾å¤‡æ˜¾ç¤º008é€šä¿¡æ•…éšœæ€ä¹ˆå¤„ç†ï¼Ÿ"
        answer = rag_agent.ask(query)
        print(f"ğŸ‘¤ ç”¨æˆ·ï¼š{query}")
        print(f"ğŸ¤– åŠ©æ‰‹ï¼š{answer}\n")

        # GraphèŠ‚ç‚¹æµ‹è¯•
        print("=== æµ‹è¯•GraphèŠ‚ç‚¹æ¨¡å¼ ===")
        rag_node = create_simple_rag_node(llm=llm_no_think)
        state = {"messages": [HumanMessage(content=query)]}
        result_state = rag_node(state)
        print(f"ğŸ¤– GraphèŠ‚ç‚¹å›å¤ï¼š{result_state['messages'][0].content}")

    except Exception as e:
        print(f"âŒ ç³»ç»Ÿè¿è¡Œå¤±è´¥ï¼š{str(e)}")
        exit(1)